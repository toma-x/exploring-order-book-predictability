{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VALOLI_9piER"
   },
   "source": [
    "# **Exploring order book predictability in cryptocurrency markets in a deep learning perspective using JAX**\n",
    "\n",
    "This notebook is used to explore order book predictability in cryptocurrency markets. We choose Bitcoin because it is the most widely known and thus the most liquid. We shine some lights on clear signs of short/mid term predictability.\n",
    "\n",
    "### **Data**\n",
    "We use the BTCUSDT book ticker, available publicly for the first day of each month on [Tardis.dev](https://docs.tardis.dev/historical-data-details/binance). It can be downloaded with the tardis-dev python library.\n",
    "\n",
    "### **References**\n",
    "[1] Zhang Z, Zohren S, Roberts S. DeepLOB: Deep convolutional neural networks for limit order books. IEEE Transactions on Signal Processing. 2019 Mar 25. https://arxiv.org/abs/1808.03668\n",
    "\n",
    "[2] Kolm, Petter N. and Turiel, Jeremy and Westray, Nicholas, Deep Order Flow Imbalance: Extracting Alpha at Multiple Horizons from the Limit Order Book. 2021 August 5. [https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3900141](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3900141)\n",
    "\n",
    "[3] Lucchese L, S.Pankkanen M, E.D.Veraart A. THE SHORT-TERM PREDICTABILITY OF RETURNS IN ORDER BOOK MARKETS: A DEEP LEARNING PERSPECTIVE. 2023 Oct 10. [https://arxiv.org/pdf/2211.13777.pdf](https://arxiv.org/pdf/2211.13777.pdf)\n",
    "\n",
    "[4] Dean Markwick. Order Flow Imbalance - A High Frequency Trading Signal. 2022 Feb 2. [https://dm13450.github.io/2022/02/02/Order-Flow-Imbalance.html](https://dm13450.github.io/2022/02/02/Order-Flow-Imbalance.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3Y9Vi1gID9ki"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ypy-websocket 0.8.2 requires aiofiles<23,>=22.1.0, but you have aiofiles 0.8.0 which is incompatible.\n",
      "gradio 4.14.0 requires aiofiles<24.0,>=22.0, but you have aiofiles 0.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas tardis-dev tqdm pyarrow clu --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F5nGeX46srTX",
    "outputId": "fa8408ff-e66c-461b-efd3-04c3b3a05f55"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tardis_dev import datasets\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "today = datetime.now()\n",
    "if today.day <= 2:\n",
    "  today = today - relativedelta(months=1)\n",
    "\n",
    "days = [datetime(today.year, today.month, 1) - i*relativedelta(months=1) for i in range(6)][::-1]\n",
    "\n",
    "for day in days:\n",
    "  datasets.download(\n",
    "      exchange=\"binance\",\n",
    "      data_types=[\"book_ticker\"],\n",
    "      from_date=day.strftime(\"%Y-%m-%d\"),\n",
    "      to_date=(day+relativedelta(days=1)).strftime(\"%Y-%m-%d\"),\n",
    "      symbols=[\"BTCUSDT\"])\n",
    "\n",
    "file_paths = [f'datasets/binance_book_ticker_{day.strftime(\"%Y-%m-%d\")}_BTCUSDT.csv.gz' for day in days]\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "8nAF-FJ8s-QN",
    "outputId": "98f7ce89-d2f9-4d6f-fcdc-cf850b301130"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "df = pd.concat([pd.read_csv(file_path, engine='pyarrow') for file_path in file_paths], axis=0)\n",
    "df.drop(['local_timestamp'], axis=1, inplace=True)\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='us')\n",
    "df.set_index('timestamp', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_dqA4rBvGyw"
   },
   "source": [
    "## **Processing the training data**\n",
    "\n",
    "We are totalizing 22M rows containing the first level of the order book.\n",
    "The data will be processed and labelled as:\n",
    "  - Sell (label 0)\n",
    "  - Hold (label 1)\n",
    "  - Buy (label 2)\n",
    "\n",
    "The label of a row is computed as follows:\n",
    "For a row at time $t$, we denote $b_t$ and $a_t$ the bid and ask price, define the sell and buy return at time $t$ and horizon $h$ as:\n",
    "$$r_t^{\\text{sell}} = \\dfrac{b_t - \\dfrac{1}{h}\\sum_{s=1}^h a_{t+s}}{b_t};\n",
    "r_t^{\\text{buy}} = \\dfrac{\\dfrac{1}{h}\\sum_{s=1}^h b_{t+s} - a_t}{a_t}$$\n",
    "\n",
    "The parameter $h$ is purely ad-hoc, it represents the size of the lookahead window. We will try to keep the number of these ad-hoc parameters as low as possible, to ensure more fitting to the training data result in more accuracy in a real market context.\n",
    "\n",
    "**Explanation**: if we sell at time $t$, $\\dfrac{1}{h}\\sum_{s=1}^h a_{t+s}$ is the smoothed ask price over h steps in the future, we want this value to be low (e.g the price descending) to buy later at a better price. $r_t^{\\text{sell}}$ is then computed by weighting this smoothed ask return with the price we sold at ($b_t$).\n",
    "The smoothing is necessary due to a lot of noise and volatilty in the cryptocurrency markets, we need to ensure the prices have made a noticeable change to label a row as buy, or sell.\n",
    "\n",
    "This definition of $r$ differs from the current litterature, in [3] Lucchese and al; used only the mid price to compute the return at time t.\n",
    "In our trading perspective, we must take into account only tradable price (e.g. the bid and ask prices)\n",
    "\n",
    "Moreover the spread, the difference between the ask and bid price, can spike to large values in periods of high volatility, making $m_t$ a worse approximation of the real market state.\n",
    "\n",
    "More definitions of returns are explored in [3]\n",
    "\n",
    "Then we introduce $\\gamma$ which is the threshold above which a row will be labelled at 0, or 2.\n",
    "\n",
    "A row is labelled as 0 (sell) if $r_t^{\\text{sell}} > \\gamma$, labelled as 2 (buy) if $r_t^{\\text{buy}} > \\gamma$, otherwise the row is labelled as 1 (hold).\n",
    "\n",
    "**Setting 0 as gamma will label a row as buy if and only if the smoothed bid price over h steps ahead beats the spread (e.g. we are making profits).**\n",
    "\n",
    "The labeled rows undergo further processing, involving the normalization of bid and ask amounts. Additionally, bid and ask prices are replaced with a weighted spread. These three features—normalized bid amount, normalized ask amount, and weighted spread—constitute the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0aJbDbTt8MM"
   },
   "outputs": [],
   "source": [
    "def process_data(temp_df, h, gamma, T):\n",
    "  print('from', temp_df.index[0], 'to', temp_df.index[-1])\n",
    "  temp_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "  temp_buy_index = pd.Series(temp_df[((temp_df['bid_price'].iloc[::-1].rolling(window=h).sum()/h).shift(1).iloc[::-1] - temp_df['ask_price']) / temp_df['ask_price'] > gamma].index)\n",
    "  temp_sell_index = pd.Series(temp_df[(temp_df['bid_price'] - (temp_df['ask_price'].iloc[::-1].rolling(window=h).sum()/h).shift(1)) / temp_df['bid_price'] > gamma].index)\n",
    "\n",
    "  temp_df.loc[:, 'spread'] = 2 * (temp_df['ask_price'] - temp_df['bid_price']) / (temp_df['ask_price'] + temp_df['bid_price'])\n",
    "  temp_df[['bid_amount', 'ask_amount']] = temp_df[['bid_amount', 'ask_amount']].div(temp_df[['bid_amount', 'ask_amount']].sum(axis=1), axis=0)\n",
    "\n",
    "  temp_buy_points = temp_buy_index[temp_buy_index.diff(-1) != -1]\n",
    "  temp_sell_points = temp_sell_index[temp_sell_index.diff(-1) != -1]\n",
    "\n",
    "  temp_buy_windows = np.stack([temp_buy_points - t for t in range(T)]).T[:, ::-1]\n",
    "  temp_sell_windows = np.stack([temp_sell_points - t for t in range(T)]).T[:, ::-1]\n",
    "\n",
    "  temp_buys = temp_df.values[:, 2:][temp_buy_windows]\n",
    "  temp_sells = temp_df.values[:, 2:][temp_sell_windows]\n",
    "\n",
    "  temp_N = min(len(temp_buys), len(temp_sells))\n",
    "\n",
    "  temp_hold_index = np.random.choice(temp_df.index.difference(temp_buy_index).difference(temp_sell_index), size=temp_N)\n",
    "  temp_hold_windows = np.stack([temp_hold_index - t for t in range(T)]).T[:, ::-1]\n",
    "  temp_holds = temp_df.values[:, 2:][temp_hold_windows]\n",
    "\n",
    "  temp_buys = temp_buys[np.random.choice(temp_buys.shape[0], temp_N, replace=False)]\n",
    "  temp_sells = temp_sells[np.random.choice(temp_sells.shape[0], temp_N, replace=False)]\n",
    "\n",
    "  print(temp_buys.shape[0], 'buys')\n",
    "  print(temp_holds.shape[0], 'holds')\n",
    "  print(temp_sells.shape[0], 'sells', end='\\n\\n')\n",
    "\n",
    "  temp_books = np.vstack((temp_buys, temp_holds, temp_sells))\n",
    "  temp_labels = np.array([2]*temp_buys.shape[0]+[1]*temp_holds.shape[0]+[0]*temp_sells.shape[0])\n",
    "\n",
    "  temp_ds = {'book': jnp.array(temp_books),\n",
    "              'label': jnp.array(temp_labels)}\n",
    "  return temp_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-hI3xzX10sL"
   },
   "source": [
    "## **The Neural Network**\n",
    "\n",
    "Inline with the current litterature, we use a CNN to efficiently capture the patterns in the order book features, leading to market changes.\n",
    "\n",
    "We use Flax and Jax to write a general CNN whose hyperparameters can be changed and tuned efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41mK5aICtWfh"
   },
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from tqdm import tqdm, trange\n",
    "from clu import parameter_overview\n",
    "from typing import Any\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "  batch_stats: Any\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"CNN Model\"\"\"\n",
    "    kernel_size_1: int\n",
    "    kernel_size_2: int\n",
    "    kernel_size_3: int\n",
    "    padding: str\n",
    "    features: int\n",
    "    dense: int\n",
    "    activation: str\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train):\n",
    "        x = nn.Conv(features=2**self.features,\n",
    "                    kernel_size=(1, self.kernel_size_1))(x)\n",
    "        x = nn.BatchNorm(use_running_average=not train)(x)\n",
    "        x = eval(f\"nn.{self.activation}(x)\")\n",
    "        x = nn.Conv(features=2**self.features,\n",
    "                    kernel_size=(self.kernel_size_2, 1),\n",
    "                    padding=self.padding)(x)\n",
    "        x = nn.BatchNorm(use_running_average=not train)(x)\n",
    "        x = eval(f\"nn.{self.activation}(x)\")\n",
    "        x = nn.Conv(features=2**self.features,\n",
    "                    kernel_size=(self.kernel_size_3, 1),\n",
    "                    padding=self.padding)(x)\n",
    "        x = nn.BatchNorm(use_running_average=not train)(x)\n",
    "        x = eval(f\"nn.{self.activation}(x)\")\n",
    "\n",
    "        x = x.reshape((x.shape[0], -1))\n",
    "\n",
    "        x = nn.Dense(features=2**self.dense)(x)\n",
    "        x = eval(f\"nn.{self.activation}(x)\")\n",
    "\n",
    "        x = nn.Dense(features=3)(x) # buy, hold or sell\n",
    "        return x\n",
    "\n",
    "@jax.jit\n",
    "def apply_model(state, books, labels):\n",
    "  \"\"\"Computes gradients, loss and accuracy for a single batch.\"\"\"\n",
    "\n",
    "  def loss_fn(params):\n",
    "    logits, updates = state.apply_fn({'params': params, 'batch_stats': state.batch_stats},\n",
    "                                     books,\n",
    "                                     train=True,\n",
    "                                     mutable=['batch_stats'])\n",
    "    one_hot = jax.nn.one_hot(labels, 3)\n",
    "    loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot))\n",
    "    return loss, (logits, updates)\n",
    "\n",
    "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "  (loss, (logits, updates)), grads = grad_fn(state.params)\n",
    "  accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "  return grads, updates, loss, accuracy\n",
    "\n",
    "@jax.jit\n",
    "def update_model(state, grads, updates):\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  state = state.replace(batch_stats=updates['batch_stats'])\n",
    "  return state\n",
    "\n",
    "def train_epoch(state, train_ds, batch_size, rng):\n",
    "  \"\"\"Train for a single epoch.\"\"\"\n",
    "  train_ds_size = len(train_ds['book'])\n",
    "  steps_per_epoch = train_ds_size // batch_size\n",
    "\n",
    "  perms = jax.random.permutation(rng, len(train_ds['book']))\n",
    "  perms = perms[: steps_per_epoch * batch_size]  # skip incomplete batch\n",
    "  perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "  epoch_loss = []\n",
    "  epoch_accuracy = []\n",
    "\n",
    "  for perm in tqdm(perms):\n",
    "    batch_books = train_ds['book'][perm, ...]\n",
    "    batch_labels = train_ds['label'][perm, ...]\n",
    "    grads, updates, loss, accuracy = apply_model(state, batch_books, batch_labels)\n",
    "    state = update_model(state, grads, updates)\n",
    "    epoch_loss.append(loss)\n",
    "    epoch_accuracy.append(accuracy)\n",
    "  train_loss = np.mean(epoch_loss)\n",
    "  train_accuracy = np.mean(epoch_accuracy)\n",
    "  return state, train_loss, train_accuracy\n",
    "\n",
    "\n",
    "def create_train_state(rng, optimizer, learning_rate, T, hparams) -> TrainState:\n",
    "  \"\"\"Creates initial `TrainState`.\"\"\"\n",
    "  cnn = CNN(**hparams)\n",
    "  variables = cnn.init(rng, jnp.ones([1, T, 3]), train=False)\n",
    "  print(parameter_overview.get_parameter_overview(variables))\n",
    "  params = variables['params']\n",
    "  batch_stats = variables['batch_stats']\n",
    "  tx = eval(f\"optax.{optimizer}({learning_rate})\")\n",
    "  return TrainState.create(apply_fn=cnn.apply, params=params, tx=tx, batch_stats=batch_stats)\n",
    "\n",
    "\n",
    "def train_and_evaluate(h, gamma, T, optimizer, learning_rate, batch_size, num_epochs, hparams) -> TrainState:\n",
    "  \"\"\"Execute model training and evaluation loop.\n",
    "\n",
    "  Returns:\n",
    "    The train state (which includes the `.params`).\n",
    "  \"\"\"\n",
    "\n",
    "  print(\"Training data:\")\n",
    "  train_df = pd.concat([pd.read_csv(train_file_path, usecols=['timestamp', 'ask_price', 'bid_price', 'ask_amount', 'bid_amount'], engine='pyarrow') for train_file_path in file_paths[:-1]], axis=0)\n",
    "  train_df['timestamp'] = pd.to_datetime(train_df['timestamp'], unit='us')\n",
    "  train_df.set_index('timestamp', inplace=True)\n",
    "  train_ds = process_data(train_df, h=h, gamma=gamma, T=T)\n",
    "\n",
    "  print(\"Test data:\")\n",
    "  test_df = pd.read_csv(file_paths[-1], usecols=['timestamp', 'ask_price', 'bid_price', 'ask_amount', 'bid_amount'], engine='pyarrow')\n",
    "  test_df['timestamp'] = pd.to_datetime(test_df['timestamp'], unit='us')\n",
    "  test_df.set_index('timestamp', inplace=True)\n",
    "  test_ds = process_data(test_df, h=h, gamma=gamma, T=T)\n",
    "\n",
    "  assert 'cuda' in str(train_ds['book'].devices())\n",
    "\n",
    "  print(\"Training on shape\", train_ds[\"book\"].shape, train_ds['label'].shape)\n",
    "  print(\"Testing on shape\", test_ds[\"book\"].shape, test_ds['label'].shape)\n",
    "\n",
    "  rng = jax.random.key(0)\n",
    "  rng, init_rng = jax.random.split(rng)\n",
    "\n",
    "  state = create_train_state(init_rng, optimizer, learning_rate, T, hparams)\n",
    "  for epoch in range(1, num_epochs + 1):\n",
    "    rng, input_rng = jax.random.split(rng)\n",
    "    state, train_loss, train_accuracy = train_epoch(state, train_ds, batch_size, input_rng)\n",
    "    _, _, test_loss, test_accuracy = apply_model(state, test_ds['book'], test_ds['label'])\n",
    "    print('epoch:% 3d, train_loss: %.4f, train_accuracy: %.2f, test_loss: %.4f,'\n",
    "          ' test_accuracy: %.2f,' % (epoch,\n",
    "                                    train_loss,\n",
    "                                    train_accuracy * 100,\n",
    "                                    test_loss,\n",
    "                                    test_accuracy * 100))\n",
    "  return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0FmyFWh2nbt"
   },
   "source": [
    "## **Hyperparameters**\n",
    "\n",
    "Hyperparameters have been found conducting a extensive hyperparameters tuning study with [Optuna](https://optuna.readthedocs.io/en/stable/index.html)\n",
    "\n",
    "Find more information about hyperparameter tuning in Jax in [Guide 4: Research Projects with JAX](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/guide4/Research_Projects_with_JAX.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46COpx4Hzs0j",
    "outputId": "f788c4d1-5437-4b7a-ef07-26f7f83039df"
   },
   "outputs": [],
   "source": [
    "h = 50\n",
    "gamma = 0\n",
    "T = 10\n",
    "\n",
    "num_epochs = 10\n",
    "learning_rate = 0.03\n",
    "batch_size = 1024\n",
    "\n",
    "hparams = {'kernel_size_1': 1,\n",
    "         'kernel_size_2': 5,\n",
    "         'kernel_size_3': 8,\n",
    "         'padding': 'SAME',\n",
    "         'features': 6,\n",
    "         'dense': 9,\n",
    "         'activation': 'hard_swish'}\n",
    "\n",
    "optimizer = \"novograd\"\n",
    "\n",
    "state = train_and_evaluate(h, gamma, T, optimizer, learning_rate, batch_size, num_epochs, hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de_0x4dyjWPk"
   },
   "source": [
    "## **Evaluation**\n",
    "\n",
    "Once we have an accurate model, we can evauate the model on the unseen test dataset, and calculate the predictions for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OxOJNMAM2Pbe"
   },
   "outputs": [],
   "source": [
    "def evaluate(state, T, test_df, test_batch_size):\n",
    "    preds = pd.DataFrame(index=test_df.index, columns=['sell', 'hold' ,'buy'])\n",
    "    sliding_view = jnp.array(np.lib.stride_tricks.sliding_window_view(test_df.values, (T, 5)).reshape(-1, T, 5))\n",
    "    for k in trange(0, sliding_view.shape[0], test_batch_size):\n",
    "        test_batch = sliding_view[k: k+test_batch_size, :, 2:]\n",
    "        logits, _ = state.apply_fn({'params': state.params, 'batch_stats': state.batch_stats}, test_batch, train=False, rngs={'dropout': jax.random.key(1)}, mutable=['batch_stats'])\n",
    "        class_probabilities = jax.nn.softmax(logits)\n",
    "        preds.iloc[k+T-1: k+T-1+class_probabilities.shape[0]] = class_probabilities\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJLUaSnR25kS"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(file_paths[-1], usecols=['timestamp', 'ask_price', 'bid_price', 'ask_amount', 'bid_amount'], engine='pyarrow')\n",
    "test_df['timestamp'] = pd.to_datetime(test_df['timestamp'], unit='us')\n",
    "test_df.set_index('timestamp', inplace=True)\n",
    "test_df.loc[:, 'spread'] = 2 * (test_df['ask_price'] - test_df['bid_price']) / (test_df['ask_price'] + test_df['bid_price'])\n",
    "test_df[['bid_amount', 'ask_amount']] = test_df[['bid_amount', 'ask_amount']].div(test_df[['bid_amount', 'ask_amount']].sum(axis=1), axis=0)\n",
    "test_batch_size = 2048*8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9FXPyUNj7Dl"
   },
   "source": [
    "\n",
    "A prediction in a triplet [sell_confidence, hold_confidence, buy_confidence] of [0, 1], we log the predictions in a separate dataframe:\n",
    "\n",
    "The first T-1 rows have no prediction since we need the last T rows to feed to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473
    },
    "id": "zz4WVtfq3LVL",
    "outputId": "be4940c4-53a9-49a5-d383-c9d163319852"
   },
   "outputs": [],
   "source": [
    "preds = evaluate(state, T, test_df, test_batch_size)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEVxNEe2IL2i"
   },
   "source": [
    "Now we will implement a trading strategy based on the predictions.\n",
    "\n",
    "We write a general function to get the associated price and amount for a buy/sell prediction.\n",
    "In the perspective of a real market application, we take into account the latency between the moment we receive the book update, and the moment our order is actually filled in the exchange.\n",
    "\n",
    "Binance offer a snippet to directly compute the latency between you and the market: [https://github.com/binance/binance-toolbox-python/blob/master/check_order_update_latency.py](https://github.com/binance/binance-toolbox-python/blob/master/check_order_update_latency.py)\n",
    "\n",
    "Binance websocket servers are located in Tokyo, we use an AWS virtual machine located in this area to get the lowest latency and the results we got are around 4 ms to receive the order book update, then the model runs inference in about 37 ms, and our order is submitted and filled in 3ms according to our tests.\n",
    "\n",
    "This brings the total latency to around 45 milliseconds, which is shockingly pedestrian in the high frequency trading realm. We are putting ourself at risk of buying or selling at a higher or lower price than the one we saw quoted. The gold standard in HFT is around 200 nanoseconds.\n",
    "\n",
    "Additionnaly, we assume that our activity doesn't impact the market, which is arguable if we start to trade large quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xH2Spmn6q1jA",
    "outputId": "98e12188-d701-4be1-d241-976876f8bdf7"
   },
   "outputs": [],
   "source": [
    "%timeit state.apply_fn({'params': state.params, 'batch_stats': state.batch_stats}, jnp.ones([1, T, 3]), train=False, mutable=['batch_stats'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vKbgBPKWIKWR"
   },
   "outputs": [],
   "source": [
    "def compute_prices(test_df, preds, latency=pd.Timedelta(milliseconds=50), confidences=np.linspace(1/3, 1, 10)):\n",
    "    pred_df = pd.concat([test_df, preds], axis=1)\n",
    "    conf_df = test_df.copy()\n",
    "    for confidence in confidences:\n",
    "        labels = pd.Series([1]*len(test_df.index), index=test_df.index, name=f'label_{round(confidence, 2)}', dtype=np.uint) # hold everything\n",
    "        labels[preds.max(axis=1) > confidence] = np.argmax(preds.values, axis=1)[preds.max(axis=1) > confidence]\n",
    "        conf_df = pd.concat([conf_df, labels], axis=1)\n",
    "\n",
    "    df_temp = conf_df.copy()\n",
    "    df_temp.index = df_temp.index - latency\n",
    "\n",
    "    # Perform an asof merge to get the last price and amount at time timestamp + latency for each timestamp, direction backward to trade at the current market state after latency\n",
    "    df_merged = pd.merge_asof(conf_df, df_temp[['bid_price', 'ask_price', 'bid_amount', 'ask_amount']], left_index=True, right_index=True, suffixes=('', '_future'), direction=\"backward\")\n",
    "\n",
    "    df_merged.drop(['ask_price', 'ask_amount', 'bid_price', 'bid_amount'], inplace=True, axis=1)\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJL-XJ_2Lj5c"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(file_paths[-1], usecols=['timestamp', 'ask_price', 'bid_price', 'ask_amount', 'bid_amount'], engine='pyarrow')\n",
    "test_df['timestamp'] = pd.to_datetime(test_df['timestamp'], unit='us')\n",
    "test_df.set_index('timestamp', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "id": "Z1sX4iUALcNd",
    "outputId": "29b113bb-baee-41c4-88e0-7ec183d2699f"
   },
   "outputs": [],
   "source": [
    "prices = compute_prices(test_df, preds)\n",
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O70sUeykS7UE"
   },
   "outputs": [],
   "source": [
    "def compute_trades(prices):\n",
    "  trades = {}\n",
    "  for label_column in prices.filter(regex=r'label_').columns:\n",
    "    confidence = float(label_column.split('_')[-1])\n",
    "    conf_df = prices[[label_column, 'bid_price_future', 'ask_price_future', 'bid_amount_future', 'ask_amount_future']].copy()\n",
    "\n",
    "    conf_df = conf_df[conf_df[label_column] != 1].dropna() # remove holds\n",
    "\n",
    "    # Remove consecutive duplicate labels\n",
    "    conf_df['group'] = (conf_df[label_column] != conf_df[label_column].shift()).cumsum()\n",
    "    conf_df = conf_df.drop_duplicates(subset='group')\n",
    "    conf_df.drop(columns='group', inplace=True)\n",
    "\n",
    "    conf_df.loc[conf_df[label_column] == 2, 'price_future'] = - conf_df.loc[conf_df[label_column] == 2, 'ask_price_future']\n",
    "    conf_df.loc[conf_df[label_column] == 0, 'price_future'] = conf_df.loc[conf_df[label_column] == 0, 'bid_price_future']\n",
    "    conf_df.loc[conf_df[label_column] == 2, 'amount_future'] = conf_df.loc[conf_df[label_column] == 2, 'ask_amount_future']\n",
    "    conf_df.loc[conf_df[label_column] == 0, 'amount_future'] = conf_df.loc[conf_df[label_column] == 0, 'bid_amount_future']\n",
    "\n",
    "    conf_df.drop(['bid_price_future', 'ask_price_future', 'bid_amount_future', 'ask_amount_future'], inplace=True, axis=1)\n",
    "    trades[confidence] = conf_df.copy()\n",
    "\n",
    "  return trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "pxmQ5xqSznW9",
    "outputId": "7dfe1c85-f87d-4fae-f925-71fec4058481"
   },
   "outputs": [],
   "source": [
    "trades = compute_trades(prices)\n",
    "next(iter(trades.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hNayqQFTy9R"
   },
   "source": [
    "For every confidence, we have the label along with price and the available quantity after latency for every signal\n",
    "\n",
    "## **The trading strategy**\n",
    "\n",
    "Define a trading strategy as a previsible process, given by the amounts (BTC_wallet, USDT_wallet)_n for each timestamp n.\n",
    "\n",
    "The updated market price is given by (price, 1)_n\n",
    "\n",
    "A trading strategy is said to be self-funded if for every n:\n",
    "\n",
    "```\n",
    "BTC_wallet_n * price_n + USDT_wallet_n = BTC_wallet_(n+1) * price_n + USDT_wallet_(n+1)\n",
    "```\n",
    "This means that the modification of the wallet composition at time n+1 is just a rearrangment of the wallet composition at time n. There is no additional funds, and no withdrawal.\n",
    "This means that the more money we will earn during the process, the more money we will trade, leading to exponential value of our wallet.\n",
    "Remember that an exponential can decrease (very fast...)\n",
    "\n",
    "The strategy we adopt is self-funded and verify that our wallet value is always non negative. Such a strategy is called eligible. This means that we won't get into dept using it, but we can lose all our money.\n",
    "\n",
    "An eligible strategy leading starting with an empty wallet and leading to positive profit is an arbitrage. This is making profit without taking any risk.\n",
    "\n",
    "As in [4], we compute the Sharpe ratio (SR) of our strategy, this metric is the expectancy of returns divided by the standard deviation of the returns, it measures how risky our strategy is compared to the yielded profits.\n",
    "\n",
    "One would demand the Sharpe ratio to be at least over 1, meaning the expectancy of results is greater than variance of returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "3XHkRYi6XyFW",
    "outputId": "87c10558-c6d9-4b85-d1f2-49f76359f031"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "for confidence, trade in trades.items():\n",
    "  if len(trade) <= 4: continue\n",
    "  BTC_wallet = 1\n",
    "  USDT_wallet = abs(trade.iloc[0]['price_future'])\n",
    "  wallets = []\n",
    "  for timestamp, (label, price, amount) in trade.iterrows():\n",
    "    amount = 1 # ignoring slippage\n",
    "    if label == 0:\n",
    "      trade_amount = min(BTC_wallet, amount)\n",
    "      BTC_wallet -= trade_amount\n",
    "      USDT_wallet += price*trade_amount\n",
    "      wallets.append((timestamp, BTC_wallet*price+USDT_wallet))\n",
    "    if label == 2:\n",
    "      price *= -1\n",
    "      trade_amount = min(USDT_wallet, price*amount)\n",
    "      BTC_wallet += trade_amount/price\n",
    "      USDT_wallet -= trade_amount\n",
    "      wallets.append((timestamp, BTC_wallet*price+USDT_wallet))\n",
    "  wallets = np.array(wallets)\n",
    "  diffs = np.diff(wallets[:, 1])\n",
    "  plt.plot(wallets[:, 0], wallets[:, 1], label=f'conf={confidence}, SR={round(diffs.mean()/diffs.std(), 2)}')\n",
    "plt.title(\"Latency, no slippage, no fee\")\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%H:%m'))\n",
    "plt.ylabel(\"USDT\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "W_lonhQkOouG",
    "outputId": "5b8d29ea-a974-4b16-97dd-e9c007bfc17e"
   },
   "outputs": [],
   "source": [
    "for confidence, trade in trades.items():\n",
    "  if not len(trade): continue\n",
    "  BTC_wallet = 1\n",
    "  USDT_wallet = abs(trade.iloc[0]['price_future'])\n",
    "  wallets = []\n",
    "  for timestamp, (label, price, amount) in trade.iterrows():\n",
    "    if label == 0:\n",
    "      trade_amount = min(BTC_wallet, amount) # slippage\n",
    "      BTC_wallet -= trade_amount\n",
    "      USDT_wallet += price*trade_amount\n",
    "      wallets.append((timestamp, BTC_wallet*price+USDT_wallet))\n",
    "    if label == 2:\n",
    "      price *= -1\n",
    "      trade_amount = min(USDT_wallet, price*amount) # slippage\n",
    "      BTC_wallet += trade_amount/price\n",
    "      USDT_wallet -= trade_amount\n",
    "      wallets.append((timestamp, BTC_wallet*price+USDT_wallet))\n",
    "  wallets = np.array(wallets)\n",
    "  diffs = np.diff(wallets[:, 1])\n",
    "  plt.plot(wallets[:, 0], wallets[:, 1], label=f'conf={confidence}, SR={round(diffs.mean()/diffs.std(), 2)}')\n",
    "plt.title(\"Latency, slippage, no fee\")\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%H:%m'))\n",
    "plt.ylabel(\"USDT\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_IU6LjgZF0r"
   },
   "source": [
    "Still winnning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "7bZtufYNMCYm",
    "outputId": "10a0f9e3-9f8b-481e-99f6-4834fe8b1858",
    "tags": []
   },
   "outputs": [],
   "source": [
    "maker_fee = 0.0090/100 # https://www.binance.com/en/fee/trading\n",
    "taker_fee = 2*maker_fee\n",
    "for confidence, trade in trades.items():\n",
    "  if not len(trade): continue\n",
    "  BTC_wallet = 1\n",
    "  USDT_wallet = abs(trade.iloc[0]['price_future'])\n",
    "  wallets = []\n",
    "  for timestamp, (label, price, amount) in trade.iterrows():\n",
    "    if label == 0:\n",
    "      trade_amount = min(BTC_wallet, amount)\n",
    "      BTC_wallet -= trade_amount\n",
    "      USDT_wallet += price*trade_amount*(1-taker_fee) # taker fee\n",
    "      wallets.append((timestamp, BTC_wallet*price+USDT_wallet))\n",
    "    if label == 2:\n",
    "      price *= -1\n",
    "      trade_amount = min(USDT_wallet, price*amount)\n",
    "      BTC_wallet += trade_amount/price*(1-maker_fee) # maker fee\n",
    "      USDT_wallet -= trade_amount\n",
    "      wallets.append((timestamp, BTC_wallet*price+USDT_wallet))\n",
    "  wallets = np.array(wallets)\n",
    "  diffs = np.diff(wallets[:, 1])\n",
    "  plt.plot(wallets[:, 0], wallets[:, 1], label=f'conf={confidence}, SR={round(diffs.mean()/diffs.std(), 2)}')\n",
    "plt.title(\"Latency, slippage, fee\")\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%H:%m'))\n",
    "plt.ylabel(\"USDT\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
